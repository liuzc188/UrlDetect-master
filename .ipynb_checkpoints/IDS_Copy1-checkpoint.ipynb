{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import urllib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各十条数据作为例子分析\n",
    "# 输入参数部分\n",
    "good = 'data/good_fromE.txt'\n",
    "\n",
    "bad = 'data/badqueries.txt'\n",
    "\n",
    "k = 80\n",
    "# ngram 系数\n",
    "n = 2\n",
    "\n",
    "# wether use kmean\n",
    "# use_k = False\n",
    "use_k = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printT(word):\n",
    "    a = time.strftime('%Y-%m-%d %H:%M:%S: ', time.localtime(time.time()))\n",
    "    print(a+str(word))\n",
    "\n",
    "\n",
    "# return[good,bad]\n",
    "def getdata():\n",
    "    with open(good, 'r') as f:\n",
    "        good_query_list = [i.strip('\\n') for i in f.readlines()[:]]\n",
    "    with open(bad, 'r') as f:\n",
    "        bad_query_list = [i.strip('\\n') for i in f.readlines()[:]]\n",
    "    return [good_query_list, bad_query_list]\n",
    "\n",
    "\n",
    "class IDS(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     训练模型基类\n",
    "class Baseframe(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def getname(self):\n",
    "        return 'baseframe'\n",
    "    \n",
    "    def Train(self):\n",
    "\n",
    "        printT('读入数据，good：'+good+' bad:'+bad)\n",
    "        data = getdata()\n",
    "        printT('done, good numbers:'+str(len(data[0]))+' bad numbers:'+str(len(data[1])))\n",
    "        # 打标记\n",
    "        good_y = [0 for i in range(len(data[0]))]\n",
    "        bad_y = [1 for i in range(len(data[1]))]\n",
    "        \n",
    "        y = good_y + bad_y\n",
    "\n",
    "        #     向量化\n",
    "        # converting data to vectors  定义矢量化实例\n",
    "        self.vectorizer = TfidfVectorizer(tokenizer=self.get_ngrams)\n",
    "        # 把不规律的文本字符串列表转换成规律的 ( [i,j],weight) 的矩阵X [url条数，分词总类的总数，理论上少于256^n] \n",
    "        # i表示第几条url，j对应于term编号（或者说是词片编号）\n",
    "        # 用于下一步训练分类器 lgs\n",
    "        X = self.vectorizer.fit_transform(data[0]+data[1])\n",
    "        printT('向量化后维度：'+str(X.shape))\n",
    "        # 通过kmeans降维 返回降维后的矩阵\n",
    "        if use_k:\n",
    "            X = self.transform(self.kmeans(X))\n",
    "\n",
    "            printT('降维完成')\n",
    "\n",
    "        printT('划分测试集训练集')\n",
    "        # 使用 train_test_split 分割 X y 列表 testsize表示测试占的比例 random为种子\n",
    "        # X_train矩阵的数目对应 y_train列表的数目(一一对应)  -->> 用来训练模型\n",
    "        # X_test矩阵的数目对应 \t (一一对应) -->> 用来测试模型的准确性\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        printT('划分完成，开始训练分类器')\n",
    "        printT(self.classifier)\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "\n",
    "        # 使用测试值 对 模型的准确度进行计算\n",
    "        printT(self.getname()+'模型的准确度:{}'.format(self.classifier.score(X_test, y_test)))\n",
    "        \n",
    "        #         保存训练结果\n",
    "        with open('model/'+self.getname()+'.pickle', 'wb') as output:\n",
    "            pickle.dump(self, output)\n",
    "\n",
    "    # 对新的请求列表进行预测\n",
    "    def predict(self, new_queries):\n",
    "        try:\n",
    "            with open('model/'+self.getname()+'.pickle', 'rb') as input:\n",
    "                self = pickle.load(input)\n",
    "            printT('loading '+self.getname()+'model success')\n",
    "        except FileNotFoundError:\n",
    "            printT('start to train the '+self.getname()+' model')\n",
    "            self.Train()\n",
    "        printT('start predict:')\n",
    "        #         解码\n",
    "        new_queries = [urllib.parse.unquote(url) for url in new_queries]\n",
    "        X_predict = self.vectorizer.transform(new_queries)\n",
    "        if use_k:\n",
    "            printT('将输入转换')\n",
    "            X_predict = self.transform(X_predict.tolil().transpose())\n",
    "\n",
    "        printT('转换完成,开始预测')\n",
    "        res = self.classifier.predict(X_predict)\n",
    "        printT('预测完成,总数：' + str(len(res)))\n",
    "        result = {}\n",
    "\n",
    "        result[0] = []\n",
    "        result[1] = []\n",
    "        \n",
    "        #         两个列表并入一个元组列表\n",
    "        for q, r in zip(new_queries, res):\n",
    "            result[r].append(q)\n",
    "\n",
    "        printT('good query: '+str(len(result[0])))\n",
    "        printT('bad query: '+str(len(result[1])))\n",
    "        printT(\"预测的结果列表:{}\".format(str(result)))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    \n",
    "# tokenizer function, this will make 3 grams of each query\n",
    "    def get_ngrams(self, query):\n",
    "        tempQuery = str(query)\n",
    "        ngrams = []\n",
    "        for i in range(0, len(tempQuery)-n):\n",
    "            ngrams.append(tempQuery[i:i+n])\n",
    "        return ngrams\n",
    "\n",
    "    def kmeans(self, weight):\n",
    "\n",
    "        printT('kmeans之前矩阵大小： ' + str(weight.shape))\n",
    "        weight = weight.tolil().transpose()\n",
    "        # 同一组数据 同一个k值的聚类结果是一样的。保存结果避免重复运算\n",
    "        try:\n",
    "\n",
    "            with open('model/k' + str(k) + '.label', 'r') as input:\n",
    "\n",
    "                printT('loading kmeans success')\n",
    "                a = input.read().split(' ')\n",
    "\n",
    "                self.label = [int(i) for i in a[:-1]]\n",
    "\n",
    "        except FileNotFoundError:\n",
    "\n",
    "            printT('Start Kmeans ')\n",
    "\n",
    "            clf = KMeans(n_clusters=k, precompute_distances=False )\n",
    "\n",
    "            s = clf.fit(weight)\n",
    "            printT(s)\n",
    "\n",
    "            # 保存聚类的结果\n",
    "            self.label = clf.labels_\n",
    "\n",
    "            # with open('model/' + self.getname() + '.kmean', 'wb') as output:\n",
    "            #     pickle.dump(clf, output)\n",
    "            with open('model/k' + str(k) + '.label', 'w') as output:\n",
    "                for i in self.label:\n",
    "                    output.write(str(i) + ' ')\n",
    "        printT('kmeans 完成,聚成 ' + str(k) + '类')\n",
    "        return weight\n",
    "\n",
    "    #     转换成聚类后结果 输入转置后的矩阵 返回转置好的矩阵\n",
    "    def transform(self, weight):\n",
    "\n",
    "        from scipy.sparse import coo_matrix\n",
    "\n",
    "        a = set()\n",
    "        # 用coo存 可以存储重复位置的元素\n",
    "        row = []\n",
    "        col = []\n",
    "        data = []\n",
    "        # i代表旧矩阵行号 label[i]代表新矩阵的行号\n",
    "        for i in range(len(self.label)):\n",
    "            if self.label[i] in a:\n",
    "                continue\n",
    "            a.add(self.label[i])\n",
    "            for j in range(i, len(self.label)):\n",
    "                if self.label[j] == self.label[i]:\n",
    "                    temp = weight[j].rows[0]\n",
    "                    col += temp\n",
    "                    temp = [self.label[i] for t in range(len(temp))]\n",
    "                    row += temp\n",
    "                    data += weight[j].data[0]\n",
    "\n",
    "        # print(row)\n",
    "        # print(col)\n",
    "        # print(data)\n",
    "        newWeight = coo_matrix((data, (row, col)), shape=(k,weight.shape[1]))\n",
    "        return newWeight.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LG(Baseframe):\n",
    "    def getname(self):\n",
    "        if use_k:\n",
    "            return 'LG__n'+str(n)+'_k'+str(k)\n",
    "        return 'LG_n'+str(n)\n",
    "\n",
    "    def __init__(self):\n",
    "        # 定理逻辑回归方法模型\n",
    "        self.classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(Baseframe):\n",
    "\n",
    "    def getname(self):\n",
    "        if use_k:\n",
    "            return 'SVM__n'+str(n)+'_k'+str(k)\n",
    "        return 'SVM_n'+str(n)\n",
    "\n",
    "    def __init__(self):\n",
    "        # 定理逻辑回归方法模型\n",
    "        self.classifier = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
